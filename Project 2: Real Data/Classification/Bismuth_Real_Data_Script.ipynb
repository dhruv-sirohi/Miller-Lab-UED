{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bismuth_Real_Data_Script.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtN8CZ2ExjiF",
        "outputId": "1e9248e7-0b85-4af1-d694-46da51cf47dd"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Tensorflow 2.x is used \n",
        "%tensorflow_version 2.x\n",
        "\n",
        "#IMPORTS\n",
        "import os\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "import random\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout,Flatten, Conv2D,MaxPooling2D\n",
        "import keras.backend as K\n",
        "\n",
        "# Training is done on a GPU\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PyDuTuCyBXG"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/Bismuth_Data_Processed/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WX4I9WSzJDU",
        "outputId": "ccf0dcab-7e0e-450f-fb39-199a8595a9ca"
      },
      "source": [
        "for file in os.scandir(data_dir):\n",
        "  print(file.path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Bismuth_Data_Processed/Bismuth Scan Notes COMPLETE - Useful scans.csv\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/processed data scan100.txt\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/Bismuth Scan Notes COMPLETE - Scans by fluence.csv\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/Bismuth Scan Notes COMPLETE - Useful scans.tsv\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/MONEYSHOT\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-25\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-21\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-17\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-16\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-12\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-18\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-19\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-22\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-24\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-09\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-11\n",
            "/content/drive/MyDrive/Bismuth_Data_Processed/2021-02-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68bEBCpMzlyv"
      },
      "source": [
        "def process_data(data_dir):\n",
        "  data_array = []\n",
        "  for subdirectories, directories, files in os.walk(data_dir):\n",
        "    for file_name in files:\n",
        "      file_loc = subdirectories + os.path.sep + file_name\n",
        "      \n",
        "      if file_loc.endswith(\".jpg\"):      \n",
        "        photo = Image.open(file_loc)\n",
        "        #bw_photo_arr = rgb2gray(np.array(photo))\n",
        "        photo_arr = np.array(photo)\n",
        "        data_array.append([photo_arr,file_loc])\n",
        "        #print(len(data_array))\n",
        "  random.shuffle(data_array)\n",
        "  print(\"Done! Added {} images\".format(len(data_array)))\n",
        "  return data_array\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq4hkrMJFtpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045c4668-64bf-40c9-af0d-65c3a9f57640"
      },
      "source": [
        "data_array = process_data(data_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! Added 7962 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtiwwAsrIEU_"
      },
      "source": [
        "photos,file_names = zip(*data_array)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp-yBy_nOcy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f09ff8-f673-449c-f39f-fa3fe7b71c82"
      },
      "source": [
        "i = 0\n",
        "labels = []\n",
        "usable_photos = []\n",
        "\n",
        "for file_num in range(0,len(file_names)):\n",
        "  processed_file_name = file_names[file_num].lower().strip().replace(' ','').split('bismuth_data_processed')[1]\n",
        "  \"\"\"\n",
        "  print(processed_file_name)\n",
        "  scan_num = processed_file_name.split('scan')[3].split('_gl')[0]\n",
        "  time = processed_file_name.split('scan',-1)[3].split('_gl')[1].split('_fs_')[0]\n",
        "  power = processed_file_name.split('scan')[3].split('_gl')[1].split('_fs_')[1].split('_')[0]\n",
        "  print(scan_num)\n",
        "  print(time)\n",
        "  print(power)\n",
        "  \"\"\"\n",
        "  if 'on' in processed_file_name:\n",
        "    labels.append([0,1])\n",
        "    usable_photos.append(photos[file_num])\n",
        "  elif 'off' in processed_file_name:\n",
        "    labels.append([1,0])\n",
        "    usable_photos.append(photos[file_num])\n",
        "#print(labels[0].shape)\n",
        "labels_array = np.array(labels).reshape(-1,2)\n",
        "print(len(labels_array))\n",
        "\n",
        "photos_array = np.array(usable_photos).reshape(-1,150,150,3)\n",
        "print(photos_array.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7614\n",
            "(7614, 150, 150, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzthB6esVHfY"
      },
      "source": [
        "# Defines the neural network that will be trained on the player's data \n",
        "# Returns uncompiled network\n",
        "def build_resnet_model(lr):\n",
        "  \"\"\"\n",
        "  Transfer learning: a ResNet50 trained on imagenet is used with its last 4 layers removed.\n",
        "  Trainable fully connected layers are added on top of the imagenet conv layers\n",
        "  Dropout was initially used but was later removed for performance reasons\n",
        "  https://towardsdatascience.com/pitfalls-with-dropout-and-batchnorm-in-regression-problems-39e02ce08e4d\n",
        "  \"\"\"\n",
        "  resnet = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "  for layer in resnet.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Prints network details for debugging\n",
        "  \"\"\"\n",
        "  for layer in resnet.layers:\n",
        "    print(layer, layer.trainable)\n",
        "  \"\"\"\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(resnet)\n",
        "  # model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation='tanh'))\n",
        "  # model.add(Dropout(0.5))\n",
        "  model.add(Dense(200, activation='tanh'))\n",
        "  # model.add(Dropout(0.5))\n",
        "  model.add(Dense(50, activation='tanh'))\n",
        "  # model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='tanh'))\n",
        "  # model.add(Dropout(0.5))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  # Compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate = lr), metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def build_alexnet():\n",
        "  #Instantiate an empty model\n",
        "  model = Sequential()\n",
        "  # 1st Convolutional Layer\n",
        "  model.add(Conv2D(filters=96, input_shape=(150,150,3), kernel_size=(10,10), strides=(2,2), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  # 2nd Convolutional Layer\n",
        "  model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  # 3rd Convolutional Layer\n",
        "  model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # 4th Convolutional Layer\n",
        "  model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # 5th Convolutional Layer\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(Activation('relu'))\n",
        "  # Max Pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  # Passing it to a Fully Connected layer\n",
        "  model.add(Flatten())\n",
        "  # 1st Fully Connected Layer\n",
        "  model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout to prevent overfitting\n",
        "  model.add(Dropout(0.6))\n",
        "  # 2nd Fully Connected Layer\n",
        "  model.add(Dense(4096))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout\n",
        "  model.add(Dropout(0.6))\n",
        "  # 3rd Fully Connected Layer\n",
        "  model.add(Dense(1000))\n",
        "  model.add(Activation('relu'))\n",
        "  # Add Dropout\n",
        "  model.add(Dropout(0.6))\n",
        "  # Output Layer\n",
        "  model.add(Dense(2))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate = 0.01), metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def build_cnn_model():\n",
        "  cnn_model = keras.Sequential([\n",
        "      keras.layers.Conv2D(filters=18,input_shape=(150,150,3),kernel_size=(5,5),activation=tf.nn.relu),           \n",
        "      keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=36,kernel_size=(5,5),activation=tf.nn.relu), \n",
        "      keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "      keras.layers.Dropout(0.6),\n",
        "      keras.layers.Dense(2, activation=tf.nn.softmax)\n",
        "  ])\n",
        "  cnn_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
        "  return cnn_model"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPG--rsIWn-S",
        "outputId": "34576a45-7425-46f8-d2de-fe81acb00b44"
      },
      "source": [
        "\n",
        "#model = build_resnet_model(0.0008)\n",
        "model = build_cnn_model()\n",
        "accuracy_values = []\n",
        "validation_accuracy_values= []\n",
        "loss_values = []\n",
        "validation_loss_values = []\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "#Training Neural Net\n",
        "for i in range(0,EPOCHS):\n",
        "  print('----- --------STARTING EPOCH {}-------------'.format(i))\n",
        "\n",
        "  history = model.fit(photos_array,labels_array,batch_size=30,epochs=1,validation_split=0.1)\n",
        "\n",
        "  #Storing accuracy and loss values\n",
        "  accuracy_values.append(history.history['accuracy'][0])\n",
        "  validation_accuracy_values.append(history.history['val_accuracy'][0])\n",
        "  loss_values.append(history.history['loss'][0])\n",
        "  validation_loss_values.append(history.history['val_loss'][0])\n",
        "\n",
        "#Plotting NN performance\n",
        "f1 = plt.figure()\n",
        "f2 = plt.figure()\n",
        "ax1 = f1.add_subplot(111)\n",
        "\n",
        "ax1.plot(accuracy_values, \"-b\",label = \"train accuracy\")\n",
        "ax1.plot(validation_accuracy_values, \"-r\",label = \"test accuracy\")\n",
        "ax1.legend()\n",
        "ax2 = f2.add_subplot(111)\n",
        "ax2.plot(loss_values, \"-b\",label = \"train loss\")\n",
        "ax2.plot(validation_loss_values, \"-r\",label = \"test loss\")\n",
        "ax2.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- --------STARTING EPOCH 0-------------\n",
            "229/229 [==============================] - 3s 11ms/step - loss: 0.9949 - accuracy: 0.5445 - val_loss: 0.6412 - val_accuracy: 0.6102\n",
            "----- --------STARTING EPOCH 1-------------\n",
            "229/229 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 0.5933 - val_loss: 0.6245 - val_accuracy: 0.6115\n",
            "----- --------STARTING EPOCH 2-------------\n",
            "229/229 [==============================] - 2s 11ms/step - loss: 0.6557 - accuracy: 0.5736 - val_loss: 0.6520 - val_accuracy: 0.5892\n",
            "----- --------STARTING EPOCH 3-------------\n",
            "229/229 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 0.5849 - val_loss: 0.6434 - val_accuracy: 0.5932\n",
            "----- --------STARTING EPOCH 4-------------\n",
            "229/229 [==============================] - 2s 11ms/step - loss: 0.6271 - accuracy: 0.5822 - val_loss: 0.6419 - val_accuracy: 0.5997\n",
            "----- --------STARTING EPOCH 5-------------\n",
            "229/229 [==============================] - 2s 11ms/step - loss: 0.6301 - accuracy: 0.5873 - val_loss: 0.6596 - val_accuracy: 0.5958\n",
            "----- --------STARTING EPOCH 6-------------\n",
            "207/229 [==========================>...] - ETA: 0s - loss: 0.6155 - accuracy: 0.6066"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}